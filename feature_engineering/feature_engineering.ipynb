{"cells":[{"cell_type":"code","metadata":{"source_hash":"f4bfa479","execution_start":1738246404077,"execution_millis":669,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"f81d684fd3564701aece01612163f02a","deepnote_cell_type":"code"},"source":"#Loading libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Load the merged dataset\nmerged_data = pd.read_csv(\"merged_data.csv\")","outputs":[],"outputs_reference":null,"execution_count":1,"block_group":"f81d684fd3564701aece01612163f02a","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"549ae03","execution_start":1738246431876,"execution_millis":1,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"f382306cc9bf4c47a3304ca15357e157","deepnote_cell_type":"code"},"source":"#Feature Engineering#\n# 1. Address Missing Data\n# Numerical columns: Continuous and Count Data\nnumerical_columns = merged_data.select_dtypes(include=['float64', 'int64']).columns\nfor col in numerical_columns:\n    if merged_data[col].isnull().sum() > 0:\n        # Use median for imputation\n        merged_data[col].fillna(merged_data[col].median(), inplace=True)\n\n# Categorical columns: Impute with the mode\ncategorical_columns = merged_data.select_dtypes(include=['object']).columns\nfor col in categorical_columns:\n    if merged_data[col].isnull().sum() > 0:\n        # Use mode for imputation\n        merged_data[col].fillna(merged_data[col].mode()[0], inplace=True)\n\n# Date columns: Forward-fill or backward-fill for imputation\ndate_columns = [col for col in merged_data.columns if 'Date' in col or 'date' in col]\nfor col in date_columns:\n    if merged_data[col].isnull().sum() > 0:\n        # Forward-fill method\n        merged_data[col] = pd.to_datetime(merged_data[col], errors='coerce')\n        merged_data[col].fillna(method='ffill', inplace=True)","outputs":[],"outputs_reference":null,"execution_count":3,"block_group":"2a32f6036adc4fa8b8d62204a6b33d5a","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"efa029b3","execution_start":1738246482497,"execution_millis":0,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"8f2c494c7dc341b2b4bc2ca0e9e21196","deepnote_cell_type":"code"},"source":"# 2. One-Hot Encoding for Categorical Columns\n# Verify available columns\nprint(\"Available columns:\", merged_data.columns)\n\n# Specify columns to encode (adjust if needed)\nspecified_categorical_columns = ['Product_Name', 'Company_Name', 'Address']\n\n# Dynamically filter to include only existing columns\ncategorical_columns_to_encode = [\n    col for col in specified_categorical_columns if col in merged_data.columns\n]\n\nif categorical_columns_to_encode:\n    # Update: Correcting the OneHotEncoder usage by removing the 'sparse_output' argument\n    # which is not valid. This behavior is controlled by 'sparse' argument. Switching accordingly.\n    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n    encoded_features = pd.DataFrame(\n        encoder.fit_transform(merged_data[categorical_columns_to_encode]),\n        columns=encoder.get_feature_names_out(categorical_columns_to_encode)\n    )\n\n    # Add encoded features to the dataset\n    merged_data = pd.concat([merged_data.reset_index(drop=True), encoded_features.reset_index(drop=True)], axis=1)\n\n    # Drop original categorical columns after encoding\n    merged_data = merged_data.drop(columns=categorical_columns_to_encode)\nelse:\n    print(\"No valid categorical columns found for one-hot encoding.\")","outputs":[{"name":"stdout","text":"Available columns: Index(['Transaction_ID', 'Company_ID', 'Product_ID', 'Quantity',\n       'Transaction_Date', 'Product_Price_x', 'Total_Cost', 'Product_Name',\n       'Product_Price_y', 'Company_Name', 'Company_Profit', 'Address'],\n      dtype='object')\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/1829397e-0619-4218-9b8f-fb97a1a9f652","execution_count":7,"block_group":"3b6eb18a4a88449faf3d4cec5b5bf809","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"19e4809","execution_start":1738246508584,"execution_millis":1,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"55edbb492978409482a2023fe464a841","deepnote_cell_type":"code"},"source":"# 3. Scale Numerical Features\n# Define numerical columns for scaling (ensure they exist)\nnumerical_columns = [\n    col for col in ['Product_Price', 'Quantity', 'Total_Cost', 'Company_Profit']\n    if col in merged_data.columns\n]\n\nif numerical_columns:\n    scaler = StandardScaler()\n    merged_data[numerical_columns] = scaler.fit_transform(merged_data[numerical_columns])\nelse:\n    print(\"No valid numerical columns found for scaling.\")\n\n# Final Check: Ensure all features are processed\nprint(\"Feature Engineering Complete. Dataset Preview:\")\nprint(merged_data.head())","outputs":[{"name":"stdout","text":"Feature Engineering Complete. Dataset Preview:\n   Transaction_ID  Company_ID  Product_ID  Quantity Transaction_Date  \\\n0             1.0        88.0         6.0  0.076948       2024-03-26   \n1             2.0        29.0        19.0  0.984139       2024-07-09   \n2             3.0        28.0        18.0 -0.830243       2024-04-13   \n3             4.0        85.0        12.0  0.258386       2023-09-06   \n4             5.0        47.0         3.0 -0.467367       2021-07-06   \n\n   Product_Price_x  Total_Cost  Product_Price_y  Company_Profit  \\\n0    194379.147964   -0.395485         179200.0       -0.013588   \n1     97930.993380    0.013660          95200.0       -0.596943   \n2    126095.547778   -0.551349         134400.0       -0.631825   \n3    131600.000000   -0.473417          84000.0        1.550027   \n4     99575.609634   -0.824112         100800.0       -1.881425   \n\n   Product_Name_BudgetMaster Pro  ...  \\\n0                            0.0  ...   \n1                            0.0  ...   \n2                            0.0  ...   \n3                            1.0  ...   \n4                            0.0  ...   \n\n   Address_Slex, barangay 404, taguig, philippines  \\\n0                                              0.0   \n1                                              0.0   \n2                                              0.0   \n3                                              0.0   \n4                                              0.0   \n\n   Address_Slex, barangay 505, manila, philippines  \\\n0                                              0.0   \n1                                              0.0   \n2                                              0.0   \n3                                              0.0   \n4                                              0.0   \n\n   Address_Slex, brgy. 202, manila, philippines  \\\n0                                           0.0   \n1                                           0.0   \n2                                           0.0   \n3                                           0.0   \n4                                           0.0   \n\n   Address_Taft Ave, Barangay 123, Manila, Philippines!  \\\n0                                                0.0      \n1                                                0.0      \n2                                                0.0      \n3                                                0.0      \n4                                                0.0      \n\n   Address_Taft Ave, Barangay 123, Pasig, Philippines  \\\n0                                                0.0    \n1                                                0.0    \n2                                                0.0    \n3                                                0.0    \n4                                                0.0    \n\n   Address_Taft Ave, Barangay 456, Cebu City, Philippines  \\\n0                                                0.0        \n1                                                0.0        \n2                                                0.0        \n3                                                0.0        \n4                                                0.0        \n\n   Address_Taft Ave, Barangay 606, Makati, Philippines!  \\\n0                                                0.0      \n1                                                0.0      \n2                                                0.0      \n3                                                0.0      \n4                                                0.0      \n\n   Address_Taft Ave, Barangay 707, Mandaluyong, Philippines  \\\n0                                                0.0          \n1                                                0.0          \n2                                                0.0          \n3                                                0.0          \n4                                                1.0          \n\n   Address_Taft ave, barangay 202, baguio, philippines  \\\n0                                                0.0     \n1                                                0.0     \n2                                                0.0     \n3                                                0.0     \n4                                                0.0     \n\n   Address_Taft ave, brgy. 456, manila, philippines  \n0                                               0.0  \n1                                               0.0  \n2                                               0.0  \n3                                               0.0  \n4                                               0.0  \n\n[5 rows x 226 columns]\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/09777559-93c4-49d4-8990-141dad9bf0e5","execution_count":9,"block_group":"a1f837732f5942f1b357ab04cda8f4d2","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1fd35f41","execution_start":1738246530063,"execution_millis":22,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"8189746f0e704900bd1fc5a20c7dc342","deepnote_cell_type":"code"},"source":"# Define X (features) and y (target)\nX = merged_data.drop(columns=['Product_ID'])\ny = merged_data['Product_ID']\n\n# Convert target variable to categorical (if necessary)\ny = y.astype(str)\n\n### Split Data ###\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","outputs":[],"outputs_reference":null,"execution_count":11,"block_group":"b14ee5c49d654302952bb444ef4aa130","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"4ffb66ee","execution_start":1738246566212,"execution_millis":83214,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"b3987e2ae3ee4229ad7884abc0bc6c1f","deepnote_cell_type":"code"},"source":"# Fix: Remove or encode non-numerical columns in the features\n# Drop non-numerical columns such as 'Transaction_Date' to avoid conversion errors\nX_numerical = X.select_dtypes(include=['number'])\n\n# Split Data (again) after correction\nX_train, X_test, y_train, y_test = train_test_split(X_numerical, y, test_size=0.2, random_state=42)\n\n# Model 1: Multinomial Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Multinomial Logistic Regression\nlogistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42, max_iter=1000)\n\n# Train the model\nlogistic_model.fit(X_train, y_train)\n\n# Predictions\ny_pred_logistic = logistic_model.predict(X_test)\n\n# Evaluation Metrics\nlogistic_metrics = {\n    \"Model\": \"Multinomial Logistic Regression\",\n    \"Accuracy\": accuracy_score(y_test, y_pred_logistic),\n    \"Classification Report\": classification_report(y_test, y_pred_logistic, output_dict=True)\n}\n\n# Print results\nprint(\"Multinomial Logistic Regression Metrics:\")\nprint(f\"Accuracy: {logistic_metrics['Accuracy']}\")\nprint(classification_report(y_test, y_pred_logistic))\n\n# Save predictions to a DataFrame\nlogistic_predictions = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred_logistic})\nlogistic_predictions.to_csv(\"logistic_predictions.csv\", index=False)","outputs":[{"name":"stderr","text":"/root/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nMultinomial Logistic Regression Metrics:\nAccuracy: 0.1465\n              precision    recall  f1-score   support\n\n         1.0       0.00      0.00      0.00        82\n        10.0       0.15      0.80      0.25       266\n        11.0       0.00      0.00      0.00        92\n        12.0       0.00      0.00      0.00        97\n        13.0       0.08      0.01      0.02        77\n        14.0       0.00      0.00      0.00        93\n        15.0       0.00      0.00      0.00        83\n        16.0       0.00      0.00      0.00       106\n        17.0       0.00      0.00      0.00        82\n        18.0       0.00      0.00      0.00        96\n        19.0       0.00      0.00      0.00        85\n         2.0       0.00      0.00      0.00        88\n        20.0       0.13      0.71      0.23        90\n         3.0       0.00      0.00      0.00        98\n         4.0       0.00      0.00      0.00        94\n         5.0       0.31      0.16      0.21        97\n         6.0       0.00      0.00      0.00       100\n         7.0       0.00      0.00      0.00        93\n         8.0       0.00      0.00      0.00        86\n         9.0       0.00      0.00      0.00        95\n\n    accuracy                           0.15      2000\n   macro avg       0.03      0.08      0.04      2000\nweighted avg       0.04      0.15      0.05      2000\n\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/0184bfbd-44ac-4f30-b9f4-63cbc772baa4","execution_count":15,"block_group":"f4e3ecd5b0aa42beb1f71cfb191961e9","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b67e682","execution_start":1738246676468,"execution_millis":1182,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"bf460760d9814f93855fbae279558e2c","deepnote_cell_type":"code"},"source":"#Model 2: Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Random Forest Classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Predictions\ny_pred_rf = rf_model.predict(X_test)\n\n# Evaluation Metrics\nrf_metrics = {\n    \"Model\": \"Random Forest Classifier\",\n    \"Accuracy\": accuracy_score(y_test, y_pred_rf),\n    \"Classification Report\": classification_report(y_test, y_pred_rf, output_dict=True)\n}\n\n# Print results\nprint(\"Random Forest Classifier Metrics:\")\nprint(f\"Accuracy: {rf_metrics['Accuracy']}\")\nprint(classification_report(y_test, y_pred_rf))\n\n# Save predictions to a DataFrame\nrf_predictions = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred_rf})\nrf_predictions.to_csv(\"rf_predictions.csv\", index=False)","outputs":[{"name":"stdout","text":"Random Forest Classifier Metrics:\nAccuracy: 1.0\n              precision    recall  f1-score   support\n\n         1.0       1.00      1.00      1.00        82\n        10.0       1.00      1.00      1.00       266\n        11.0       1.00      1.00      1.00        92\n        12.0       1.00      1.00      1.00        97\n        13.0       1.00      1.00      1.00        77\n        14.0       1.00      1.00      1.00        93\n        15.0       1.00      1.00      1.00        83\n        16.0       1.00      1.00      1.00       106\n        17.0       1.00      1.00      1.00        82\n        18.0       1.00      1.00      1.00        96\n        19.0       1.00      1.00      1.00        85\n         2.0       1.00      1.00      1.00        88\n        20.0       1.00      1.00      1.00        90\n         3.0       1.00      1.00      1.00        98\n         4.0       1.00      1.00      1.00        94\n         5.0       1.00      1.00      1.00        97\n         6.0       1.00      1.00      1.00       100\n         7.0       1.00      1.00      1.00        93\n         8.0       1.00      1.00      1.00        86\n         9.0       1.00      1.00      1.00        95\n\n    accuracy                           1.00      2000\n   macro avg       1.00      1.00      1.00      2000\nweighted avg       1.00      1.00      1.00      2000\n\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/ba055d95-39be-4bbb-853b-7e8f14f68c94","execution_count":17,"block_group":"093d7acd56984583b1e5caece22f2a9e","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"e4cdd076","execution_start":1738246705280,"execution_millis":163,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"52fec72b3f134dfb883adda2f65d00d9","deepnote_cell_type":"code"},"source":"#Model Comparison Matrix\n# Consolidate metrics into a single table\ncomparison_table = pd.DataFrame({\n    \"Model\": [logistic_metrics[\"Model\"], rf_metrics[\"Model\"]],\n    \"Accuracy\": [logistic_metrics[\"Accuracy\"], rf_metrics[\"Accuracy\"]],\n    \"Precision (Weighted Avg)\": [\n        logistic_metrics[\"Classification Report\"][\"weighted avg\"][\"precision\"],\n        rf_metrics[\"Classification Report\"][\"weighted avg\"][\"precision\"]\n    ],\n    \"Recall (Weighted Avg)\": [\n        logistic_metrics[\"Classification Report\"][\"weighted avg\"][\"recall\"],\n        rf_metrics[\"Classification Report\"][\"weighted avg\"][\"recall\"]\n    ],\n    \"F1-Score (Weighted Avg)\": [\n        logistic_metrics[\"Classification Report\"][\"weighted avg\"][\"f1-score\"],\n        rf_metrics[\"Classification Report\"][\"weighted avg\"][\"f1-score\"]\n    ]\n})\n\n# Save comparison table to CSV\ncomparison_table.to_csv(\"model_comparison.csv\", index=False)\n\n# Print the comparison table\nprint(\"Model Comparison Table:\")\nprint(comparison_table)","outputs":[{"name":"stdout","text":"Model Comparison Table:\n                             Model  Accuracy  Precision (Weighted Avg)  \\\n0  Multinomial Logistic Regression    0.1465                  0.043627   \n1         Random Forest Classifier    1.0000                  1.000000   \n\n   Recall (Weighted Avg)  F1-Score (Weighted Avg)  \n0                 0.1465                 0.054319  \n1                 1.0000                 1.000000  \n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/5725f04a-0292-4ce5-a2b8-03b3174ba5fa","execution_count":19,"block_group":"c4dccefe439b4060920837faf9c7771f","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"66da47f9","execution_start":1738246738052,"execution_millis":1,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"53eed1d6d1194ad9abeed6f1508c1c2b","deepnote_cell_type":"code"},"source":"# Predict on the same sample data\nsample_data = X_test.iloc[:12]  # Predicting 12 datapoints\nlogistic_predictions = logistic_model.predict(sample_data)\nrf_predictions = rf_model.predict(sample_data)\n\n# Combine predictions into a single DataFrame\ncomparison_df = pd.DataFrame({\n    'Sample Index': sample_data.index,\n    'Actual': y_test.loc[sample_data.index].values,\n    'Logistic Regression Prediction': logistic_predictions,\n    'Random Forest Prediction': rf_predictions\n})\n\n# Display the comparison table\nprint(\"\\nComparison of Predictions Side by Side:\")\nprint(comparison_df)","outputs":[{"name":"stdout","text":"\nComparison of Predictions Side by Side:\n    Sample Index Actual Logistic Regression Prediction  \\\n0           6252   19.0                           10.0   \n1           4684   10.0                           10.0   \n2           1731   10.0                           10.0   \n3           4742    3.0                           10.0   \n4           4521    3.0                           10.0   \n5           6340    9.0                           20.0   \n6            576   17.0                           10.0   \n7           5202    3.0                           10.0   \n8           6363   14.0                           10.0   \n9            439    9.0                           20.0   \n10          2750    1.0                           10.0   \n11          7487    3.0                           10.0   \n\n   Random Forest Prediction  \n0                      19.0  \n1                      10.0  \n2                      10.0  \n3                       3.0  \n4                       3.0  \n5                       9.0  \n6                      17.0  \n7                       3.0  \n8                      14.0  \n9                       9.0  \n10                      1.0  \n11                      3.0  \n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/04e95b86-385e-4234-b6c9-5ef1b7739874","execution_count":21,"block_group":"6b8d27ee07df4f3c8c47da286fd01d09","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"a796ef5","execution_start":1738246806796,"execution_millis":383,"execution_context_id":"f67a6328-c7e9-4f19-b34c-113f3cdbc209","cell_id":"268ddf2fa8064547b54a4b4176ddeb7a","deepnote_cell_type":"code"},"source":"from sklearn.metrics import accuracy_score, classification_report\n\n# Correctly reference the RandomForestClassifier object defined previously\nrf_pred_probs = rf_model.predict_proba(X_test)  # Changed 'random_forest_model' to 'rf_model'\n\n# --- Printing the predictions side by side ---\n# Create a DataFrame for the predictions to print them side by side\nlogistic_pred_df = pd.DataFrame(logistic_model.predict_proba(X_test), columns=[f'Product_{i}' for i in range(1, len(logistic_model.classes_) + 1)])  # Fixed logistic_pred_probs\nrf_pred_df = pd.DataFrame(rf_pred_probs, columns=[f'Product_{i}' for i in range(1, len(rf_model.classes_) + 1)])\n\n# Print the predicted probabilities from both models side by side\ncomparison_df = pd.concat([logistic_pred_df, rf_pred_df], axis=1, keys=[\"Logistic Regression\", \"Random Forest\"])\nprint(\"\\nPredicted Probabilities from Both Models (Logistic Regression vs Random Forest):\")\nprint(comparison_df)\n\n# --- Accuracy and Metrics ---\n# Predict classes for evaluation (select the product with the highest probability)\nlogistic_preds = logistic_model.predict(X_test)\nrf_preds = rf_model.predict(X_test)  # Changed 'random_forest_model' to 'rf_model'\n\n# Calculate accuracy scores\nlogistic_accuracy = accuracy_score(y_test, logistic_preds)\nrf_accuracy = accuracy_score(y_test, rf_preds)\n\nprint(f\"\\nLogistic Regression Accuracy: {logistic_accuracy:.4f}\")\nprint(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n\n# --- Display Classification Reports ---\nprint(\"\\nLogistic Regression Classification Report:\")\nprint(classification_report(y_test, logistic_preds))\n\nprint(\"\\nRandom Forest Classification Report:\")\nprint(classification_report(y_test, rf_preds))","outputs":[{"name":"stdout","text":"\nPredicted Probabilities from Both Models (Logistic Regression vs Random Forest):\n     Logistic Regression                                                    \\\n               Product_1 Product_2 Product_3 Product_4 Product_5 Product_6   \n0               0.054202  0.092478  0.043681  0.041381  0.040047  0.045196   \n1               0.057921  0.176506  0.061720  0.014107  0.047302  0.049999   \n2               0.055280  0.212913  0.072135  0.010461  0.044329  0.054319   \n3               0.055180  0.100047  0.050900  0.032042  0.044442  0.048006   \n4               0.047289  0.125183  0.039666  0.054538  0.022790  0.047520   \n...                  ...       ...       ...       ...       ...       ...   \n1995            0.043210  0.089345  0.030660  0.075929  0.020228  0.040487   \n1996            0.059453  0.152139  0.057209  0.011266  0.057753  0.042386   \n1997            0.022788  0.176597  0.011589  0.135436  0.002767  0.028135   \n1998            0.051655  0.054457  0.070244  0.008649  0.100583  0.041652   \n1999            0.045174  0.092197  0.031877  0.070323  0.022259  0.040858   \n\n                                               ... Random Forest             \\\n     Product_7 Product_8 Product_9 Product_10  ...    Product_11 Product_12   \n0     0.050089  0.052331  0.049574   0.051430  ...          0.94        0.0   \n1     0.029482  0.034388  0.054613   0.052713  ...          0.00        0.0   \n2     0.027428  0.029983  0.056681   0.049459  ...          0.00        0.0   \n3     0.047132  0.048425  0.056130   0.050717  ...          0.01        0.0   \n4     0.066042  0.056548  0.041467   0.045765  ...          0.03        0.0   \n...        ...       ...       ...        ...  ...           ...        ...   \n1995  0.073253  0.062006  0.036819   0.042255  ...          0.99        0.0   \n1996  0.026911  0.033696  0.063697   0.050205  ...          0.00        0.0   \n1997  0.071682  0.049881  0.008870   0.027959  ...          0.00        0.0   \n1998  0.020891  0.026342  0.076575   0.043076  ...          0.00        0.0   \n1999  0.070038  0.061442  0.038575   0.043895  ...          0.05        0.0   \n\n                                                                        \\\n     Product_13 Product_14 Product_15 Product_16 Product_17 Product_18   \n0          0.00       0.01       0.00       0.01       0.00        0.0   \n1          0.00       0.00       0.00       0.00       0.03        0.0   \n2          0.00       0.00       0.00       0.00       0.02        0.0   \n3          0.00       0.89       0.00       0.00       0.00        0.0   \n4          0.01       0.86       0.00       0.00       0.00        0.0   \n...         ...        ...        ...        ...        ...        ...   \n1995       0.00       0.00       0.00       0.01       0.00        0.0   \n1996       0.00       0.00       0.00       0.00       0.92        0.0   \n1997       0.00       0.01       0.00       0.00       0.00        0.0   \n1998       0.00       0.00       0.00       0.01       0.00        0.0   \n1999       0.00       0.01       0.01       0.00       0.00        0.0   \n\n                            \n     Product_19 Product_20  \n0           0.0       0.00  \n1           0.0       0.01  \n2           0.0       0.00  \n3           0.0       0.02  \n4           0.0       0.02  \n...         ...        ...  \n1995        0.0       0.00  \n1996        0.0       0.00  \n1997        0.0       0.03  \n1998        0.0       0.00  \n1999        0.0       0.00  \n\n[2000 rows x 40 columns]\n\nLogistic Regression Accuracy: 0.1465\nRandom Forest Accuracy: 1.0000\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n         1.0       0.00      0.00      0.00        82\n        10.0       0.15      0.80      0.25       266\n        11.0       0.00      0.00      0.00        92\n        12.0       0.00      0.00      0.00        97\n        13.0       0.08      0.01      0.02        77\n        14.0       0.00      0.00      0.00        93\n        15.0       0.00      0.00      0.00        83\n        16.0       0.00      0.00      0.00       106\n        17.0       0.00      0.00      0.00        82\n        18.0       0.00      0.00      0.00        96\n        19.0       0.00      0.00      0.00        85\n         2.0       0.00      0.00      0.00        88\n        20.0       0.13      0.71      0.23        90\n         3.0       0.00      0.00      0.00        98\n         4.0       0.00      0.00      0.00        94\n         5.0       0.31      0.16      0.21        97\n         6.0       0.00      0.00      0.00       100\n         7.0       0.00      0.00      0.00        93\n         8.0       0.00      0.00      0.00        86\n         9.0       0.00      0.00      0.00        95\n\n    accuracy                           0.15      2000\n   macro avg       0.03      0.08      0.04      2000\nweighted avg       0.04      0.15      0.05      2000\n\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n         1.0       1.00      1.00      1.00        82\n        10.0       1.00      1.00      1.00       266\n        11.0       1.00      1.00      1.00        92\n        12.0       1.00      1.00      1.00        97\n        13.0       1.00      1.00      1.00        77\n        14.0       1.00      1.00      1.00        93\n        15.0       1.00      1.00      1.00        83\n        16.0       1.00      1.00      1.00       106\n        17.0       1.00      1.00      1.00        82\n        18.0       1.00      1.00      1.00        96\n        19.0       1.00      1.00      1.00        85\n         2.0       1.00      1.00      1.00        88\n        20.0       1.00      1.00      1.00        90\n         3.0       1.00      1.00      1.00        98\n         4.0       1.00      1.00      1.00        94\n         5.0       1.00      1.00      1.00        97\n         6.0       1.00      1.00      1.00       100\n         7.0       1.00      1.00      1.00        93\n         8.0       1.00      1.00      1.00        86\n         9.0       1.00      1.00      1.00        95\n\n    accuracy                           1.00      2000\n   macro avg       1.00      1.00      1.00      2000\nweighted avg       1.00      1.00      1.00      2000\n\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/99b88530-53b0-4778-9dd7-6c726997ca8b","execution_count":25,"block_group":"d0889d811b2e473b94895a335638681a","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6a672b84-99aa-4a43-8d4f-014dc3397bc0' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"08ebbbf5eb134fc8842cabe25be9fce0"}}